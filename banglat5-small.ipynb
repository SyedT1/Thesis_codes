{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7449173,"sourceType":"datasetVersion","datasetId":4336043},{"sourceId":1382412,"sourceType":"datasetVersion","datasetId":806606}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets tqdm pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-21T19:14:00.255957Z","iopub.execute_input":"2024-01-21T19:14:00.256320Z","iopub.status.idle":"2024-01-21T19:14:14.049773Z","shell.execute_reply.started":"2024-01-21T19:14:00.256290Z","shell.execute_reply":"2024-01-21T19:14:14.048599Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:14:25.855625Z","iopub.execute_input":"2024-01-21T19:14:25.855937Z","iopub.status.idle":"2024-01-21T19:14:37.550501Z","shell.execute_reply.started":"2024-01-21T19:14:25.855910Z","shell.execute_reply":"2024-01-21T19:14:37.549280Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:14:37.553058Z","iopub.execute_input":"2024-01-21T19:14:37.553404Z","iopub.status.idle":"2024-01-21T19:14:49.224750Z","shell.execute_reply.started":"2024-01-21T19:14:37.553369Z","shell.execute_reply":"2024-01-21T19:14:49.223734Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:14:49.226160Z","iopub.execute_input":"2024-01-21T19:14:49.226478Z","iopub.status.idle":"2024-01-21T19:15:01.128552Z","shell.execute_reply.started":"2024-01-21T19:14:49.226451Z","shell.execute_reply":"2024-01-21T19:15:01.127332Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:01.130083Z","iopub.execute_input":"2024-01-21T19:15:01.130387Z","iopub.status.idle":"2024-01-21T19:15:01.675695Z","shell.execute_reply.started":"2024-01-21T19:15:01.130341Z","shell.execute_reply":"2024-01-21T19:15:01.674819Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:01.676951Z","iopub.execute_input":"2024-01-21T19:15:01.678214Z","iopub.status.idle":"2024-01-21T19:15:02.756905Z","shell.execute_reply.started":"2024-01-21T19:15:01.678177Z","shell.execute_reply":"2024-01-21T19:15:02.755625Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Sun Jan 21 19:15:02 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   61C    P0              28W /  70W |   3321MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   34C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:11.207634Z","iopub.execute_input":"2024-01-21T19:15:11.208668Z","iopub.status.idle":"2024-01-21T19:15:12.364328Z","shell.execute_reply.started":"2024-01-21T19:15:11.208629Z","shell.execute_reply":"2024-01-21T19:15:12.363333Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nimport datasets\n\ndef set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:14.125703Z","iopub.execute_input":"2024-01-21T19:15:14.126454Z","iopub.status.idle":"2024-01-21T19:15:14.134109Z","shell.execute_reply.started":"2024-01-21T19:15:14.126419Z","shell.execute_reply":"2024-01-21T19:15:14.133133Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:16.399259Z","iopub.execute_input":"2024-01-21T19:15:16.399887Z","iopub.status.idle":"2024-01-21T19:15:16.404442Z","shell.execute_reply.started":"2024-01-21T19:15:16.399853Z","shell.execute_reply":"2024-01-21T19:15:16.403403Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/80-20-rev/train8020.csv')\ndf.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:31.728612Z","iopub.execute_input":"2024-01-21T19:15:31.729009Z","iopub.status.idle":"2024-01-21T19:15:31.978588Z","shell.execute_reply.started":"2024-01-21T19:15:31.728981Z","shell.execute_reply":"2024-01-21T19:15:31.977683Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(20084, 10)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import (\n    T5ForConditionalGeneration, AutoTokenizer,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n  )\n\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:34.928764Z","iopub.execute_input":"2024-01-21T19:15:34.929441Z","iopub.status.idle":"2024-01-21T19:15:46.162308Z","shell.execute_reply.started":"2024-01-21T19:15:34.929403Z","shell.execute_reply":"2024-01-21T19:15:46.161482Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def calc_token_len(example):\n    return len(tokenizer(example).input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:15:59.640290Z","iopub.execute_input":"2024-01-21T19:15:59.640692Z","iopub.status.idle":"2024-01-21T19:15:59.645489Z","shell.execute_reply.started":"2024-01-21T19:15:59.640661Z","shell.execute_reply":"2024-01-21T19:15:59.644330Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/80-20-rev/train8020.csv')\ntest_df = pd.read_csv('/kaggle/input/80-20-rev/test8020.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:16:01.404872Z","iopub.execute_input":"2024-01-21T19:16:01.405236Z","iopub.status.idle":"2024-01-21T19:16:01.700136Z","shell.execute_reply.started":"2024-01-21T19:16:01.405207Z","shell.execute_reply":"2024-01-21T19:16:01.699120Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_name = 'csebuetnlp/banglat5_small'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:16:07.918247Z","iopub.execute_input":"2024-01-21T19:16:07.919095Z","iopub.status.idle":"2024-01-21T19:16:15.275968Z","shell.execute_reply.started":"2024-01-21T19:16:07.919065Z","shell.execute_reply":"2024-01-21T19:16:15.275015Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d585aaa69ffa4bea8d6e38e0f85e1adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f633d070d7c492c9df4410d8e52f869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4963f1a67da7415d825ab4ca10c7fd71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16011a115b154762828842fdab6fc27d"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c715d329fa4cdab762d6c1ef9aa1f2"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install git+https://github.com/csebuetnlp/normalizer","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:16:16.764537Z","iopub.execute_input":"2024-01-21T19:16:16.764874Z","iopub.status.idle":"2024-01-21T19:16:37.364869Z","shell.execute_reply.started":"2024-01-21T19:16:16.764849Z","shell.execute_reply":"2024-01-21T19:16:37.363856Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/csebuetnlp/normalizer\n  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-3oqnsrb8\n  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-3oqnsrb8\n  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from normalizer==0.0.1) (2023.8.8)\nCollecting emoji==1.4.2 (from normalizer==0.0.1)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy==6.0.3 (from normalizer==0.0.1)\n  Downloading ftfy-6.0.3.tar.gz (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\nBuilding wheels for collected packages: normalizer, emoji, ftfy\n  Building wheel for normalizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6857 sha256=a396b05414f9eb65a246a1531b89f745a1413b07c8763bb062209bc0f4d03a9c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-7bt4opfa/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186459 sha256=aeb0edd8a36faec48630b6b2d9a7125db16f05e685b537b660e61a6bd71fb965\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41930 sha256=10a31d513eb12701a51e282e84097496938cb6edbaef21bf2ef09bb558de1d4e\n  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\nSuccessfully built normalizer emoji ftfy\nInstalling collected packages: emoji, ftfy, normalizer\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.9.0\n    Uninstalling emoji-2.9.0:\n      Successfully uninstalled emoji-2.9.0\nSuccessfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from normalizer import normalize\ntrain_df['Comment'] = train_df['Comment'].apply(normalize)\ntrain_df['Correct Form'] = train_df['Correct Form'].apply(normalize)\ntest_df['Comment'] = test_df['Comment'].apply(normalize)\ntest_df['Correct Form'] = test_df['Correct Form'].apply(normalize)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:16:47.475118Z","iopub.execute_input":"2024-01-21T19:16:47.475512Z","iopub.status.idle":"2024-01-21T19:16:58.685898Z","shell.execute_reply.started":"2024-01-21T19:16:47.475479Z","shell.execute_reply":"2024-01-21T19:16:58.685031Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:05.518032Z","iopub.execute_input":"2024-01-21T19:17:05.518442Z","iopub.status.idle":"2024-01-21T19:17:05.525789Z","shell.execute_reply.started":"2024-01-21T19:17:05.518408Z","shell.execute_reply":"2024-01-21T19:17:05.524659Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"((20084, 10), (5022, 10))"},"metadata":{}}]},{"cell_type":"code","source":"def calc_token_len(example):\n    return len(tokenizer(example).input_ids)\n\ntrain_df['input_token_len'] = train_df['Comment'].apply(calc_token_len)\ntest_df['input_token_len'] = test_df['Comment'].apply(calc_token_len)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:07.570201Z","iopub.execute_input":"2024-01-21T19:17:07.570584Z","iopub.status.idle":"2024-01-21T19:17:11.009411Z","shell.execute_reply.started":"2024-01-21T19:17:07.570551Z","shell.execute_reply":"2024-01-21T19:17:11.008371Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"      Video ID        Channel name     Time of Publishing  \\\n0  62d6PfZxky0  Bangla Bhuter Golpo  2023-01-16T13:00:48Z   \n1  xgMi3vUaw5Y              Baseera  2022-04-02T19:03:14Z   \n2  8Q7mPmy9eTI  BANGLADESHI REACTOR  2023-01-22T07:00:02Z   \n3  gMeeozaMmIM         Laser Vision  2018-01-13T11:04:17Z   \n4  Vhj-_avEjs0         Ritu Hossain  2023-06-26T12:00:08Z   \n\n                                                                                                 Title  \\\n0  Bhuter Cartoon - Railway Station at 2am Night (True Story) Train Horror Story | Bangla Bhuter Golpo   \n1                                                               সিরাহ ৭ – অ্যাবিসিনিয়া | Bangla Seerah   \n2       লাইভে খেজুর পাতার জা*ইঙ্গা বিক্রি করছে টনি আপা | Salman Muqtadir female version | দাদি দাদা কই   \n3   Surja Dighal Bari | Bangla Movie | Dolly Anwar | Zahirul Haque | Rowshan Jamil | Sheikh Niamat Ali   \n4               রিতু নিজের টাকায় রুম ডেকোরেশন করলো | My Home Tour VLOG | Ritu Hossain | Rakib Hossain   \n\n            Genre  \\\n0  Entertainment    \n1   Miscellaneous   \n2   Entertainment   \n3   Entertainment   \n4   Entertainment   \n\n                                                                                                                                                                                                                                     Comment  \\\n0                                                                                                                                                                      কথা হচ্ছে ওরা এতো ট্রেন পেল কোথায় 🙄 যে এক ঘন্টা পর পর ট্রেন আসতেছে 😳   \n1                                                                                          প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন সুনে আমরা আরো বেশি উপকৃর্ত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।   \n2                                                                                                                     আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে রিপ্লাই দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।   \n3  এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়ছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂   \n4                                                                                                                                                                                        ওওআপি আমি তোমার রোম দেখার জন্য অনেক অনেক অনেক হেপি😊   \n\n   Error         Category  \\\n0      1  Multiple Errors   \n1      1         Spelling   \n2      1   Code Switching   \n3      1         Spelling   \n4      1         Spelling   \n\n                                                                                                                                                                                                                                 Correct Form  \\\n0                                                                                                                                                                          কথা হচ্ছে ওরা এত ট্রেন পেল কোথায় 🙄 যে এক ঘণ্টা পর পর ট্রেন আসছে 😳   \n1                                                                                             প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন শুনে আমরা আরো বেশি উপকৃত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।   \n2                                                                                                                         আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে জবাব দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।   \n3  এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়েছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂   \n4                                                                                                                                                                                          আপু আমি তোমার কক্ষ দেখার জন্য অনেক অনেক অনেক খুশি😊   \n\n   Unnamed: 9  input_token_len  \n0         NaN               20  \n1         NaN               37  \n2         NaN               24  \n3         NaN               55  \n4         NaN               16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video ID</th>\n      <th>Channel name</th>\n      <th>Time of Publishing</th>\n      <th>Title</th>\n      <th>Genre</th>\n      <th>Comment</th>\n      <th>Error</th>\n      <th>Category</th>\n      <th>Correct Form</th>\n      <th>Unnamed: 9</th>\n      <th>input_token_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>62d6PfZxky0</td>\n      <td>Bangla Bhuter Golpo</td>\n      <td>2023-01-16T13:00:48Z</td>\n      <td>Bhuter Cartoon - Railway Station at 2am Night (True Story) Train Horror Story | Bangla Bhuter Golpo</td>\n      <td>Entertainment</td>\n      <td>কথা হচ্ছে ওরা এতো ট্রেন পেল কোথায় 🙄 যে এক ঘন্টা পর পর ট্রেন আসতেছে 😳</td>\n      <td>1</td>\n      <td>Multiple Errors</td>\n      <td>কথা হচ্ছে ওরা এত ট্রেন পেল কোথায় 🙄 যে এক ঘণ্টা পর পর ট্রেন আসছে 😳</td>\n      <td>NaN</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xgMi3vUaw5Y</td>\n      <td>Baseera</td>\n      <td>2022-04-02T19:03:14Z</td>\n      <td>সিরাহ ৭ – অ্যাবিসিনিয়া | Bangla Seerah</td>\n      <td>Miscellaneous</td>\n      <td>প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন সুনে আমরা আরো বেশি উপকৃর্ত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন শুনে আমরা আরো বেশি উপকৃত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।</td>\n      <td>NaN</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8Q7mPmy9eTI</td>\n      <td>BANGLADESHI REACTOR</td>\n      <td>2023-01-22T07:00:02Z</td>\n      <td>লাইভে খেজুর পাতার জা*ইঙ্গা বিক্রি করছে টনি আপা | Salman Muqtadir female version | দাদি দাদা কই</td>\n      <td>Entertainment</td>\n      <td>আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে রিপ্লাই দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে জবাব দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।</td>\n      <td>NaN</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gMeeozaMmIM</td>\n      <td>Laser Vision</td>\n      <td>2018-01-13T11:04:17Z</td>\n      <td>Surja Dighal Bari | Bangla Movie | Dolly Anwar | Zahirul Haque | Rowshan Jamil | Sheikh Niamat Ali</td>\n      <td>Entertainment</td>\n      <td>এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়ছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়েছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂</td>\n      <td>NaN</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Vhj-_avEjs0</td>\n      <td>Ritu Hossain</td>\n      <td>2023-06-26T12:00:08Z</td>\n      <td>রিতু নিজের টাকায় রুম ডেকোরেশন করলো | My Home Tour VLOG | Ritu Hossain | Rakib Hossain</td>\n      <td>Entertainment</td>\n      <td>ওওআপি আমি তোমার রোম দেখার জন্য অনেক অনেক অনেক হেপি😊</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>আপু আমি তোমার কক্ষ দেখার জন্য অনেক অনেক অনেক খুশি😊</td>\n      <td>NaN</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntest_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:16.411956Z","iopub.execute_input":"2024-01-21T19:17:16.412995Z","iopub.status.idle":"2024-01-21T19:17:16.502935Z","shell.execute_reply.started":"2024-01-21T19:17:16.412958Z","shell.execute_reply":"2024-01-21T19:17:16.501911Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Video ID', 'Channel name ', 'Time of Publishing', 'Title', 'Genre', 'Comment', 'Error', 'Category', 'Correct Form', 'Unnamed: 9', 'input_token_len'],\n    num_rows: 5022\n})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass GrammarDataset(Dataset):\n    def __init__(self, dataset, tokenizer,print_text=False):\n        self.dataset = dataset\n        self.pad_to_max_length = False\n        self.tokenizer = tokenizer\n        self.print_text = print_text\n        self.max_len = 64\n\n    def __len__(self):\n        return len(self.dataset)\n\n\n    def tokenize_data(self, example):\n        input_, target_ = example['Comment'], example['Correct Form']\n\n        # tokenize inputs\n        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length,\n                                            max_length=self.max_len,\n                                            return_attention_mask=True)\n\n        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length,\n                                            max_length=self.max_len,\n                                            return_attention_mask=True)\n\n        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n            \"attention_mask\": tokenized_inputs['attention_mask'],\n            \"labels\": tokenized_targets['input_ids']\n        }\n\n        return inputs\n\n\n    def __getitem__(self, index):\n        inputs = self.tokenize_data(self.dataset[index])\n\n        if self.print_text:\n            for k in inputs.keys():\n                print(k, len(inputs[k]))\n\n        return inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:18.306026Z","iopub.execute_input":"2024-01-21T19:17:18.306686Z","iopub.status.idle":"2024-01-21T19:17:18.316191Z","shell.execute_reply.started":"2024-01-21T19:17:18.306645Z","shell.execute_reply":"2024-01-21T19:17:18.315109Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dataset = GrammarDataset(test_dataset, tokenizer, True)\nprint(dataset[121])","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:21.324203Z","iopub.execute_input":"2024-01-21T19:17:21.325058Z","iopub.status.idle":"2024-01-21T19:17:21.334543Z","shell.execute_reply.started":"2024-01-21T19:17:21.325027Z","shell.execute_reply":"2024-01-21T19:17:21.333570Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"input_ids 24\nattention_mask 24\nlabels 24\n{'input_ids': [290, 406, 79, 14, 541, 38, 4576, 2082, 1068, 5, 6673, 2008, 17, 30, 881, 840, 11777, 756, 376, 520, 3157, 20, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [290, 406, 79, 14, 541, 38, 4576, 2082, 1068, 5, 158, 2630, 16332, 30, 881, 840, 11777, 756, 376, 520, 3157, 20, 2, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:24.123139Z","iopub.execute_input":"2024-01-21T19:17:24.123905Z","iopub.status.idle":"2024-01-21T19:17:38.401851Z","shell.execute_reply.started":"2024-01-21T19:17:24.123871Z","shell.execute_reply":"2024-01-21T19:17:38.400785Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=7693d834e0f0b84e21fa28a37db1ac2829cec1cdbab0cc93ea76bd1771f27437\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nrouge_metric = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:40.979942Z","iopub.execute_input":"2024-01-21T19:17:40.980975Z","iopub.status.idle":"2024-01-21T19:17:41.447719Z","shell.execute_reply.started":"2024-01-21T19:17:40.980935Z","shell.execute_reply":"2024-01-21T19:17:41.446763Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e979f04b8c145b280929779af47a67f"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:50.252534Z","iopub.execute_input":"2024-01-21T19:17:50.253457Z","iopub.status.idle":"2024-01-21T19:17:50.258865Z","shell.execute_reply.started":"2024-01-21T19:17:50.253422Z","shell.execute_reply":"2024-01-21T19:17:50.257962Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:17:51.675771Z","iopub.execute_input":"2024-01-21T19:17:51.676130Z","iopub.status.idle":"2024-01-21T19:18:03.940404Z","shell.execute_reply.started":"2024-01-21T19:17:51.676103Z","shell.execute_reply":"2024-01-21T19:18:03.939405Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:18:03.942273Z","iopub.execute_input":"2024-01-21T19:18:03.942601Z","iopub.status.idle":"2024-01-21T19:18:16.964006Z","shell.execute_reply.started":"2024-01-21T19:18:03.942573Z","shell.execute_reply":"2024-01-21T19:18:16.962624Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.25.0\n    Uninstalling accelerate-0.25.0:\n      Successfully uninstalled accelerate-0.25.0\nSuccessfully installed accelerate-0.26.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining training related arguments\nbatch_size = 16\nargs = Seq2SeqTrainingArguments(output_dir=\"weights\",\n                        evaluation_strategy=\"steps\",\n                        per_device_train_batch_size=batch_size,\n                        per_device_eval_batch_size=batch_size,\n                        learning_rate=2e-5,\n                        num_train_epochs=100,\n                        weight_decay=0.01,\n                        save_total_limit=2,\n                        predict_with_generate=True,\n                        fp16 = True,\n                        gradient_accumulation_steps = 6,\n                        eval_steps = 500,\n                        save_steps = 500,\n                        load_best_model_at_end=True,\n                        logging_dir=\"/logs\",\n                        report_to=\"wandb\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:18:16.965584Z","iopub.execute_input":"2024-01-21T19:18:16.965907Z","iopub.status.idle":"2024-01-21T19:18:16.977972Z","shell.execute_reply.started":"2024-01-21T19:18:16.965879Z","shell.execute_reply":"2024-01-21T19:18:16.977073Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=False)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:18:16.980325Z","iopub.execute_input":"2024-01-21T19:18:16.980616Z","iopub.status.idle":"2024-01-21T19:18:16.990761Z","shell.execute_reply.started":"2024-01-21T19:18:16.980593Z","shell.execute_reply":"2024-01-21T19:18:16.989776Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining trainer using 🤗\ntrainer = Seq2SeqTrainer(model=model,\n                args=args,\n                train_dataset= GrammarDataset(train_dataset, tokenizer),\n                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n                compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:18:17.750160Z","iopub.execute_input":"2024-01-21T19:18:17.750453Z","iopub.status.idle":"2024-01-21T19:18:17.759666Z","shell.execute_reply.started":"2024-01-21T19:18:17.750428Z","shell.execute_reply":"2024-01-21T19:18:17.758900Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T19:18:17.760765Z","iopub.execute_input":"2024-01-21T19:18:17.761047Z","iopub.status.idle":"2024-01-21T22:55:14.103543Z","shell.execute_reply.started":"2024-01-21T19:18:17.761015Z","shell.execute_reply":"2024-01-21T22:55:14.102551Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240121_191856-n501wt5c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/striking_ratio/huggingface/runs/n501wt5c' target=\"_blank\">splendid-flower-12</a></strong> to <a href='https://wandb.ai/striking_ratio/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/striking_ratio/huggingface' target=\"_blank\">https://wandb.ai/striking_ratio/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/striking_ratio/huggingface/runs/n501wt5c' target=\"_blank\">https://wandb.ai/striking_ratio/huggingface/runs/n501wt5c</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10400' max='10400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10400/10400 3:35:44, Epoch 99/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.328500</td>\n      <td>1.827368</td>\n      <td>2.011900</td>\n      <td>0.807900</td>\n      <td>2.006200</td>\n      <td>2.023700</td>\n      <td>10.906200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.746600</td>\n      <td>1.559524</td>\n      <td>2.360400</td>\n      <td>0.858500</td>\n      <td>2.356300</td>\n      <td>2.369200</td>\n      <td>11.781600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.473800</td>\n      <td>1.255186</td>\n      <td>3.068900</td>\n      <td>0.970900</td>\n      <td>3.060400</td>\n      <td>3.068600</td>\n      <td>14.314400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.643000</td>\n      <td>0.364188</td>\n      <td>3.997300</td>\n      <td>1.406000</td>\n      <td>3.988400</td>\n      <td>3.997600</td>\n      <td>14.128400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.450100</td>\n      <td>0.337451</td>\n      <td>4.042800</td>\n      <td>1.441700</td>\n      <td>4.038000</td>\n      <td>4.048500</td>\n      <td>14.103100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.413200</td>\n      <td>0.324088</td>\n      <td>4.062800</td>\n      <td>1.463900</td>\n      <td>4.058100</td>\n      <td>4.066800</td>\n      <td>14.094400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.389600</td>\n      <td>0.312370</td>\n      <td>4.021300</td>\n      <td>1.444000</td>\n      <td>4.015500</td>\n      <td>4.025700</td>\n      <td>14.079600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.373600</td>\n      <td>0.306396</td>\n      <td>4.026700</td>\n      <td>1.450000</td>\n      <td>4.021600</td>\n      <td>4.028600</td>\n      <td>14.074700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.361500</td>\n      <td>0.299929</td>\n      <td>4.041300</td>\n      <td>1.450000</td>\n      <td>4.033400</td>\n      <td>4.037300</td>\n      <td>14.065700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.351500</td>\n      <td>0.294832</td>\n      <td>4.048400</td>\n      <td>1.450000</td>\n      <td>4.042800</td>\n      <td>4.047400</td>\n      <td>14.062700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.341600</td>\n      <td>0.292012</td>\n      <td>4.048400</td>\n      <td>1.450000</td>\n      <td>4.042800</td>\n      <td>4.047400</td>\n      <td>14.056800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.335700</td>\n      <td>0.288167</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.050200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.328200</td>\n      <td>0.285856</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.047400</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.324100</td>\n      <td>0.283893</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.044800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.319600</td>\n      <td>0.281728</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.042800</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.316700</td>\n      <td>0.280562</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.040000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.312800</td>\n      <td>0.279362</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.039400</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.311900</td>\n      <td>0.279064</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.039000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.309800</td>\n      <td>0.278356</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.037800</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.308800</td>\n      <td>0.277998</td>\n      <td>4.069100</td>\n      <td>1.471800</td>\n      <td>4.063800</td>\n      <td>4.067600</td>\n      <td>14.036800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10400, training_loss=0.5763367168719952, metrics={'train_runtime': 13015.7689, 'train_samples_per_second': 154.305, 'train_steps_per_second': 0.799, 'total_flos': 2.577231451521024e+16, 'train_loss': 0.5763367168719952, 'epoch': 99.36})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model('bangla_gec_model')","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:43.527405Z","iopub.execute_input":"2023-09-10T22:18:43.528408Z","iopub.status.idle":"2023-09-10T22:18:44.103809Z","shell.execute_reply.started":"2023-09-10T22:18:43.528369Z","shell.execute_reply":"2023-09-10T22:18:44.102657Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:43.512722Z","iopub.execute_input":"2023-09-10T22:18:43.515575Z","iopub.status.idle":"2023-09-10T22:18:43.525243Z","shell.execute_reply.started":"2023-09-10T22:18:43.515528Z","shell.execute_reply":"2023-09-10T22:18:43.523995Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel_name = 'bangla_gec_model'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\ndef correct_grammar(input_text,num_return_sequences,input_len):\n  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=input_len, return_tensors=\"pt\").to(torch_device)\n  translated = model.generate(**batch,max_length=input_len,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n  return tgt_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:50.332424Z","iopub.execute_input":"2023-09-10T22:18:50.332985Z","iopub.status.idle":"2023-09-10T22:18:51.442908Z","shell.execute_reply.started":"2023-09-10T22:18:50.332950Z","shell.execute_reply":"2023-09-10T22:18:51.441611Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"references,predictions = [],[]\ntest_d = test_df[test_df['Error']==1]\ntest_d_sentence = test_d['Comment'].tolist()\ntest_d_len = test_d['input_token_len'].tolist()\ntest_d_ground = test_d['Correct Form'].tolist()\nimport nltk\nnltk.download('punkt')\nfrom nltk.util import ngrams\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:51.447789Z","iopub.execute_input":"2023-09-10T22:18:51.451539Z","iopub.status.idle":"2023-09-10T22:18:51.661236Z","shell.execute_reply.started":"2023-09-10T22:18:51.451500Z","shell.execute_reply":"2023-09-10T22:18:51.660207Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate(reference,hypothesis):\n    # Tokenize sentences into words\n    reference_tokens = nltk.word_tokenize(reference)\n    hypothesis_tokens = nltk.word_tokenize(hypothesis)\n\n    # Create n-grams for reference and hypothesis\n    reference_1grams = list(ngrams(reference_tokens, 1))\n    hypothesis_1grams = list(ngrams(hypothesis_tokens, 1))\n    reference_2grams = list(ngrams(reference_tokens, 2))\n    hypothesis_2grams = list(ngrams(hypothesis_tokens, 2))\n\n    # Calculate ROUGE scores\n    rouge1_precision = len(set(reference_1grams).intersection(hypothesis_1grams)) / len(reference_1grams)\n    rouge1_recall = len(set(reference_1grams).intersection(hypothesis_1grams)) / len(hypothesis_1grams)\n    rouge2_precision = len(set(reference_2grams).intersection(hypothesis_2grams)) / len(reference_2grams)\n    rouge2_recall = len(set(reference_2grams).intersection(hypothesis_2grams)) / len(hypothesis_2grams)\n\n    # Calculate ROUGE-L using NLTK's sentence_bleu function\n    smooth = SmoothingFunction().method4\n    rougeL = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smooth)\n    d = {\n        \"rouge1_precision\":rouge1_precision,\n        \"rouge1_recall\":rouge1_recall,\n        \"rouge2_precision\":rouge2_precision,\n        \"rouge2_recall\":rouge2_recall,\n        \"rouge_l\":rougeL\n    }\n    return d","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:51.662571Z","iopub.execute_input":"2023-09-10T22:18:51.665640Z","iopub.status.idle":"2023-09-10T22:18:51.680500Z","shell.execute_reply.started":"2023-09-10T22:18:51.665600Z","shell.execute_reply":"2023-09-10T22:18:51.679401Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n\n# Function to calculate ROUGE-1, ROUGE-2, and ROUGE-L scores for a pair of texts\ndef calculate_rouge_scores(reference_tokens, system_tokens):\n    def lcs(X, Y):\n        m, n = len(X), len(Y)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if X[i - 1] == Y[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        return dp[m][n]\n\n    # Calculate ROUGE-1 (unigram) scores\n    reference_unigrams = set(reference_tokens)\n    system_unigrams = set(system_tokens)\n    overlap_rouge1 = len(reference_unigrams.intersection(system_unigrams))\n    precision_rouge1 = overlap_rouge1 / len(system_unigrams)\n    recall_rouge1 = overlap_rouge1 / len(reference_unigrams)\n    r1_t = 1 if precision_rouge1 + recall_rouge1 == 0 else 0\n    f1_rouge1 = 2 * (precision_rouge1 * recall_rouge1) / (precision_rouge1 + recall_rouge1 + r1_t)\n\n    # Calculate ROUGE-2 (bigram) scores\n    reference_bigrams = set(ngrams(reference_tokens, 2))\n    system_bigrams = set(ngrams(system_tokens, 2))\n    overlap_rouge2 = len(reference_bigrams.intersection(system_bigrams))\n    precision_rouge2 = overlap_rouge2 / len(system_bigrams)\n    recall_rouge2 = overlap_rouge2 / len(reference_bigrams)\n    r2_t = 1 if precision_rouge2 + recall_rouge2 == 0 else 1\n    f1_rouge2 = 2 * (precision_rouge2 * recall_rouge2) / (precision_rouge2 + recall_rouge2 + r2_t)\n\n    # Calculate ROUGE-L scores\n    lcs_length = lcs(reference_tokens, system_tokens)\n    precision_rougeL = lcs_length / len(system_tokens)\n    recall_rougeL = lcs_length / len(reference_tokens)\n    rL_t = 1 if precision_rougeL + recall_rougeL == 0 else 0\n    f1_rougeL = 2 * (precision_rougeL * recall_rougeL) / (precision_rougeL + recall_rougeL + rL_t)\n\n    return {\n        'ROUGE-1 Precision': precision_rouge1,\n        'ROUGE-1 Recall': recall_rouge1,\n        'ROUGE-1 F1': f1_rouge1,\n        'ROUGE-2 Precision': precision_rouge2,\n        'ROUGE-2 Recall': recall_rouge2,\n        'ROUGE-2 F1': f1_rouge2,\n        'ROUGE-L Precision': precision_rougeL,\n        'ROUGE-L Recall': recall_rougeL,\n        'ROUGE-L F1': f1_rougeL,\n    }\n\n# Function to calculate the average of ROUGE scores for an array of text pairs\ndef calculate_average_rouge_scores(reference_texts, system_texts):\n    total_scores = {\n        'ROUGE-1 Precision': 0,\n        'ROUGE-1 Recall': 0,\n        'ROUGE-1 F1': 0,\n        'ROUGE-2 Precision': 0,\n        'ROUGE-2 Recall': 0,\n        'ROUGE-2 F1': 0,\n        'ROUGE-L Precision': 0,\n        'ROUGE-L Recall': 0,\n        'ROUGE-L F1': 0,\n    }\n\n    num_pairs = len(reference_texts)\n\n    for i in range(num_pairs):\n        reference_text = reference_texts[i]\n        system_text = system_texts[i]\n\n        reference_tokens = nltk.word_tokenize(reference_text)\n        system_tokens = nltk.word_tokenize(system_text)\n\n        scores = calculate_rouge_scores(reference_tokens, system_tokens)\n\n        for key, value in scores.items():\n            total_scores[key] += value\n\n    # Calculate the average scores\n    average_scores = {key: value / num_pairs for key, value in total_scores.items()}\n    \n    return average_scores\n\n# Example usage with an array of reference and system texts\nreference_texts = test_d_ground\nsystem_texts = [correct_grammar(test_d_sentence[i],num_return_sequences=2,input_len=test_d_len[i])[0] for i in range(len(test_d_sentence)) ]\n\naverage_scores = calculate_average_rouge_scores(reference_texts, system_texts)\nprint(\"Average ROUGE Scores:\")\nfor key, value in average_scores.items():\n    print(key + \": {:.4f}\".format(value))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:18:51.685503Z","iopub.execute_input":"2023-09-10T22:18:51.687200Z","iopub.status.idle":"2023-09-10T22:20:41.518696Z","shell.execute_reply.started":"2023-09-10T22:18:51.687166Z","shell.execute_reply":"2023-09-10T22:20:41.517705Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Average ROUGE Scores:\nROUGE-1 Precision: 0.8357\nROUGE-1 Recall: 0.8343\nROUGE-1 F1: 0.8343\nROUGE-2 Precision: 0.7073\nROUGE-2 Recall: 0.7052\nROUGE-2 F1: 0.4246\nROUGE-L Precision: 0.8361\nROUGE-L Recall: 0.8342\nROUGE-L F1: 0.8344\n","output_type":"stream"}]},{"cell_type":"code","source":"# random data\n\nr = train_df['Comment'].tolist()[:20] + test_df['Comment'].tolist()[:20]\nr_ln = train_df['input_token_len'].tolist()[:20] + test_df['input_token_len'].tolist()[:20]\nreference_texts = r\nfor i in range(len(r)):\n    print(f'[input:   ] {r[i]} and [output:   ] {correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:20:41.520358Z","iopub.execute_input":"2023-09-10T22:20:41.520993Z","iopub.status.idle":"2023-09-10T22:20:46.163003Z","shell.execute_reply.started":"2023-09-10T22:20:41.520958Z","shell.execute_reply":"2023-09-10T22:20:46.161968Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"[input:   ] কাদের কি খেলব কাদের তো খেলতেই পারে না and [output:   ] কাদের কি খেলব কাদের তো খেলতেই পারে না\n[input:   ] এসব করে আরো কোন ঠাসা হবে and [output:   ] এসব করে আরো কোন ঠাসা হবে\n[input:   ] যুগ যুগ ধরে আমাদের মনে গেথে থাকবে এ গান and [output:   ] যুগ যুগ ধরে আমাদের মনে থাকবে এ গান\n[input:   ] আচছা আপু এলাজী থাকলে টিকা নেওয়া জাবেনা and [output:   ] আচছা আপু এলাজী থাকলে টিকা নেওয়া যাবেনা\n[input:   ] হে আল্লাহ এই জালিমদের থেকে আমাদের সন্তান সন্তদের কে আপনি হেফাজত করেন and [output:   ] হে আল্লাহ এই জালিমদের থেকে আমাদের সন্তান সন্তদের কে আপনি হেফাজত করেন\n[input:   ] আমার মতে মাহমুদুল্লাহ রিয়াদ বাংলাদেশের সেরা খেলোয়াড় and [output:   ] আমার মতে মাহমুদুল্লাহ রিয়াদ বাংলাদেশের সেরা খেলোয়াড়\n[input:   ] কথা গুলো শুনে কান্না করে দিয়েছি কারণ আমি ও এতীম এই পৃথিবীতে এতীম কোন দাম নেই পৃথিবীর মানুষ গুলো বড়ো স্বার্থ পর and [output:   ] কথা গুলো শুনে কান্না করে দিয়েছি কারণ আমি ও এতীম এই পৃথিবীতে এতীম কোন দাম নেই পৃথিবীর মানুষ গুলো বড়ো স্বার্থ পর\n[input:   ] ইয়া আল্লাহ্ তুমিই রক্ষার মালিক and [output:   ] ইয়া আল্লাহ্ তুমিই রক্ষার মালিক\n[input:   ] রিছারনছনের গোল বেশি সুন্দর হয়েছে and [output:   ] রিছারনছনের গোল বেশি সুন্দর হয়েছে\n[input:   ] স্বামি এত দেয় তারপর ও মন ভরেনা এরকম আছে অনেক মেয়ে জারা স্বামী কে শেষ করে আর মা বাাবাকে বাচায় and [output:   ] স্বামি এত দেয় তারপর ও মন ভরেনা এরকম আছে অনেক মেয়ে জারা স্বামী কে শেষ করে আর মাবাকে বাচায়\n[input:   ] আমাকে মনে হচ্ছে বাংলাদেশ নিম্নে ৬ টা ম্যাচ জিতবে এবার নমান ভাই and [output:   ] আমার মনে হচ্ছে বাংলাদেশ নিম্নে ৬ টা ম্যাচ জিতবে এবার নমান ভাই\n[input:   ] নাটক টা চমৎকার হয়েছে তবে এমন মেয়ে ভূল বুঝে ফিরে আসে অন্য মেয়ে গুলো ঠিকই চলে যায় and [output:   ] নাটক টা চমৎকার হয়েছে তবে এমন মেয়ে ভূল বুঝে ফিরে আসে অন্য মেয়ে গুলো ঠিকই চলে যায়\n[input:   ] আর আমরা বেগম পাড়ায় বাড়ী করছি and [output:   ] আর আমরা বেগম পাড়ায় বাড়ী করছি\n[input:   ] দোয়া করি আল্লাহ যেন শহীদ প্রেসিডেন্ট জিয়াউর রহমান কে জান্নাতুল ফেরদৌসের উচ্চ মোকাম দান করুন আমিন সুম্মা আমিন and [output:   ] দোয়া করি আল্লাহ যেন শহীদ প্রেসিডেন্ট জিয়াউর রহমান কে জান্নাতুল ফেরদৌসের উচ্চ মোকাম দান করুন আমিন সুম্মা আমিন\n[input:   ] মাহমুদুল্লাহ রিয়াদ জন্য শুভ কামনা কারিয়ারের শেষ জেনো ভালো কিছু উপহার দেয় বাংলাদেশে কে and [output:   ] মাহমুদুল্লাহ রিয়াদ জন্য শুভ কামনা কারিয়ারের শেষ জেনো ভালো কিছু উপহার দেয় বাংলাদেশে কে\n[input:   ] ছোট বেলা থেকে এখনো শুনি and [output:   ] ছোট বেলা থেকে এখনো শুনি\n[input:   ] প্রবাসী ভাইদের অনুরোধ করছি যতদিন না ধোকাবাজ হাসিনা খমতা না ছাড়বে ব্যাংকের মাধ্যমে টাকা দেওয়া বন্ধ করুন and [output:   ] প্রবাসী ভাইদের অনুরোধ করছি যতদিন না ধোকাবাজ হাসিনা খমতা না ছাড়বে ব্যাংকের মাধ্যমে টাকা দেওয়া বন্ধ করুন\n[input:   ] আলহামদুলিল্লাহ ৬ লক্ষ টাকা লস and [output:   ] আলহামদুলিল্লাহ ৬ লক্ষ টাকা লস\n[input:   ] এই পুরো ফোনের ব্যাপারটা প্রত্যেকটা কথা পজেটিভ আলোচনাটা ভীষন ভালো লাগলো and [output:   ] এই পুরো ফোনের ব্যাপারটা প্রত্যেকটা কথা পজেটিভ আলোচনাটা ভীষন ভালো লাগলো\n[input:   ] চরমোনাই আরো পরে আসবে and [output:   ] চরমোনাই আরো পরে আসবে\n[input:   ] আওয়ামী লীগের এতো লোক তাহলে কেন এত ভয় পায় and [output:   ] আওয়ামী লীগের এতো লোক তাহলে কেন এত ভয় পায়\n[input:   ] প্রাইস টা বললে কি হতো রে ইমন and [output:   ] প্রাইস টা বললে কি হতো রে ইমন\n[input:   ] এত সুন্দর হাসি ভালো লাগল and [output:   ] এত সুন্দর হাসি ভালো লাগল\n[input:   ] সময় চলে এসেছে আপনাদেরকে বিদায় জানাবার and [output:   ] সময় চলে এসেছে আপনাদেরকে বিদায় জানানোবার\n[input:   ] চুল একটু বড় হলে ভালো হত না and [output:   ] চুল একটু বড় হলে ভালো হত না\n[input:   ] সমস্যা হলো সকল সরকারি হসপিটালে অন্টিভেনোম না পাওয়া and [output:   ] সমস্যা হলো সকল সরকারি হসপিটালে অন্টিভেনোম না পাওয়া\n[input:   ] চেয়ার খেলা শুরু করে খেলার উদ্বোধন করলেন প্রধানমন্ত্রী লোকেরা মোজা পাইছি কাদের কাউয়া and [output:   ] চেয়ার খেলা শুরু করলেন প্রধানমন্ত্রী লোকেরা মোজা পাইছি কাদের কাউয়া\n[input:   ] খুব মিস করি সেইসব ছোট বেলার সময় and [output:   ] খুব মিস করি সেইসব ছোট বেলার সময়\n[input:   ] হাজার শেষ্ট করেও ইন্দিয়া এইরকম একটা ছবি করতে পারবে না এককথায় অসাধারণ and [output:   ] হাজার শেষ করেও ইন্দিয়া এইরকম একটা ছবি করতে পারবে না এককথায় অসাধারণ\n[input:   ] ভালোবাসা ভয়ঙ্কর সুন্দর and [output:   ] ভালোবাসা ভয়ঙ্কর সুন্দর\n[input:   ] ইনশাআল্লাহ জান্নাতি বান্দা এমনি হয় and [output:   ] ইনশাআল্লাহ জান্নাতি বান্দা এমনি হয়\n[input:   ] তোমাদে নিয়ে আমাদের অনেক আশা আরো বড় হও দোয়া করি and [output:   ] তোমাদে নিয়ে আমাদের অনেক আশা আরো বড় হও দোয়া করি\n[input:   ] এরকম ভিডিও দেওয়ার জন্য থ্যাংক ইউ and [output:   ] এরকম ভিডিও দেওয়ার জন্য ধন্যবাদ\n[input:   ] নাটক টা খুব অসাধারন হয়েছে and [output:   ] নাটক টা খুব অসাধারন হয়েছে\n[input:   ] আলহামদুলিল্লাহ আল্লাহ চাইলে সব সম্ভব and [output:   ] আলহামদুলিল্লাহ আল্লাহ চাইলে সব সম্ভব\n[input:   ] এই সাংবাদিক ভাই আপনার জন্য অনেক অনেক দোয়া রইলো আল্লাহ আপনাকে নেক হায়াত দান করুন আমিন আমিন and [output:   ] এই সাংবাদিক ভাই আপনার জন্য অনেক অনেক দোয়া রইলো আল্লাহ আপনাকে নেক হায়াত দান করুন আমিন আমিন\n[input:   ] আসসালামু আলাইকুম কেমন আছেন ভাইয়া আপনার জাহাজে উঠতে মন ছায় and [output:   ] আসসালামু আলাইকুম কেমন আছেন ভাইয়া আপনার জাহাজে উঠতে মন চায়\n[input:   ] খুব সুন্দর হয়ছে সিনেমা টা and [output:   ] খুব সুন্দর হয়েছে সিনেমা টা\n[input:   ] বেশি বেশি অভিজান পরিচালনা করা উচিত and [output:   ] বেশি বেশি অভিজান পরিচালনা করা উচিত\n[input:   ] আল্লাহতালা জো বাইডেন কে ইসলামের জন্য কবুল and [output:   ] আল্লাহতালা জো বাইডেন কে ইসলামের জন্য কবুল\n","output_type":"stream"}]},{"cell_type":"code","source":"# error data\n\nimport time\n\nr = (test_df[test_df['Error']==1])['Comment'].tolist()\nr_ln = (test_df[test_df['Error']==1])['input_token_len'].tolist()\nreference_texts = r\nstart_time = time.time()\nfor i in range(len(r)):\n    x = r[i]\n    y = correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]\n#     print(f'[input:   ] {r[i]} and [output:   ] {correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]}')\nend_time = time.time()\nnum_iterations = len(r)\naverage_inference_time = (end_time - start_time) / num_iterations\nprint(f\"Total Inference Time: {end_time-start_time}\")\nprint(f\"Average Inference Time: {average_inference_time:.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:22:33.803639Z","iopub.execute_input":"2023-09-10T22:22:33.804521Z","iopub.status.idle":"2023-09-10T22:24:21.829032Z","shell.execute_reply.started":"2023-09-10T22:22:33.804485Z","shell.execute_reply":"2023-09-10T22:24:21.828012Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Total Inference Time: 108.0009708404541\nAverage Inference Time: 0.1281 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"r_gn = (test_df[test_df['Error']==1])['Correct Form'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:24:21.833924Z","iopub.execute_input":"2023-09-10T22:24:21.834688Z","iopub.status.idle":"2023-09-10T22:24:21.845569Z","shell.execute_reply.started":"2023-09-10T22:24:21.834649Z","shell.execute_reply":"2023-09-10T22:24:21.844524Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"predicted_score,predicted_sentences = [],[]\nfor i in range(len(r_gn)):\n    x = r_gn[i]\n    y = correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]\n    predicted_sentences.append(y)\n    rouge_score_v = calculate_rouge_scores(x, y)\n    predicted_score.append(rouge_score_v['ROUGE-L F1'])\n# test_df['ROUGE-L F1'] = predicted_sentence\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:28:02.214360Z","iopub.execute_input":"2023-09-10T22:28:02.214736Z","iopub.status.idle":"2023-09-10T22:29:53.297564Z","shell.execute_reply.started":"2023-09-10T22:28:02.214699Z","shell.execute_reply":"2023-09-10T22:29:53.296289Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"e_df = test_df[test_df['Error']==1]\ne_df['ROUGE-L F1'] = predicted_score\ne_df['Predicted Form'] = predicted_sentences\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:29:53.326099Z","iopub.execute_input":"2023-09-10T22:29:53.326413Z","iopub.status.idle":"2023-09-10T22:29:53.337565Z","shell.execute_reply.started":"2023-09-10T22:29:53.326387Z","shell.execute_reply":"2023-09-10T22:29:53.336654Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/2934693448.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  e_df['ROUGE-L F1'] = predicted_score\n/tmp/ipykernel_28/2934693448.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  e_df['Predicted Form'] = predicted_sentences\n","output_type":"stream"}]},{"cell_type":"code","source":"e_df.sort_values(by='ROUGE-L F1', inplace=True,ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:29:53.338887Z","iopub.execute_input":"2023-09-10T22:29:53.339228Z","iopub.status.idle":"2023-09-10T22:29:53.353092Z","shell.execute_reply.started":"2023-09-10T22:29:53.339177Z","shell.execute_reply":"2023-09-10T22:29:53.351825Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3169983084.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  e_df.sort_values(by='ROUGE-L F1', inplace=True,ascending=False)\n","output_type":"stream"}]},{"cell_type":"code","source":"e_df.head(20)[1:]","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:29:53.394374Z","iopub.execute_input":"2023-09-10T22:29:53.394740Z","iopub.status.idle":"2023-09-10T22:29:53.425861Z","shell.execute_reply.started":"2023-09-10T22:29:53.394704Z","shell.execute_reply":"2023-09-10T22:29:53.424571Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"                                                                                               Video Title  \\\n858                                                  শেখ মুজিব হত্যার পর জেনারেল জিয়া যে মন্তব্য করেছিলেন   \n874                     বাধ্য হয়েই সুযোগ দেয়া হয়েছে তামিমকে ? | Tamim Iqbal | Riyad | Cricket | Ekattor TV   \n872                                           তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla   \n871   আলু দিয়ে মুরগির ঝোল বানানোর সেরা পদ্ধতি| New style chicken curry recipe in bengali |Atanur Rannaghar   \n869                      আইপিএলের নিলাম তালিকায় পাঁচ বাংলাদেশি | IPL | Bangladeshi | Khelajog | Ekattor TV   \n309                                         Turkey earthquake: সিরিয়ার ধ্বংসস্তূপ থেকে সুস্থ নবজাতক উদ্ধার   \n1542  বাহুবলী সিনেমার শ্যুটিং হয়েছিলো কেরালার যে জলপ্রপাতে || Bahubali Waterfalls | Athirapilly Waterfalls   \n1841                          Oporadhi | Ankur Mahamud Feat Arman Alif | Bangla Song 2018 | Official Video   \n312                           করোনা ভাইরাস ভ্যাকসিন: টিকা নিতে আগ্রহী হলে যেসব বিষয় আপনার জানা থাকা জরুরি   \n860                                           তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla   \n852                                                                  Khulna Travel Vlog | খুলনা ভ্রমন গল্প   \n971                               যশোরে গ্রামবাসীরা তৈরি করছে হাজার ফুট দীর্ঘ ভাসমান সেতু| BBC News Bangla   \n1544                                          তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla   \n325      Barishal । বরিশাল । Barishal Travel Vlog । History of Barishal Bangladesh । Barishal Vromon Guide   \n849                                           বিএনপির সঙ্গে ৩২টি দল- সরকারের জন্য কতটা হুমকি? | BBC Bangla   \n1547                              ডিবি হারুনের প্রশংসা করলেন হিরো আলম | Hero Alom | DB Police | Channel 24   \n844                ডলারকে বাদ দেয়ার জন্য কি কি চেষ্টা করে যাচ্ছে ব্রিকস জোট ? | Dollar | News | Ekattor TV   \n842        বিএনপির সমাবেশে গণগ্রেফতার নিয়ে যে বার্তা দিল যুক্তরাষ্ট্র | BNP | USA | BD Politics | ATN News   \n838                                         ছায়াপথ বা গ্যালাক্সি | কি কেন কিভাবে | Galaxy | Ki Keno Kivabe   \n\n              Genre  \\\n858        Politics   \n874          Sports   \n872            News   \n871   Miscellaneous   \n869          Sports   \n309            News   \n1542  Entertainment   \n1841  Entertainment   \n312            News   \n860            News   \n852   Entertainment   \n971            News   \n1544           News   \n325   Entertainment   \n849        Politics   \n1547           News   \n844        Politics   \n842        Politics   \n838   Miscellaneous   \n\n                                                                                                                                                                                                                                                                                          Comment  \\\n858                                                                                                                                                                                                                                                         আমাকে এজমা আছে আমি কি টিকা দিতে পারবো   \n874                                                                                                                                                                                                                                                 ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে দরলেন   \n872                                                                                                                                                                                                                                            স্টার জলসা আর জি বাংলা নষ্ট করলো আমাকে সোনার বাংলা   \n871                                                                                                                                                                                                                                          দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবা   \n869                                                                                                                                                                                                                                                 আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহো নেই   \n309   ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুসের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই   \n1542                                                                                                                                                                                                                 সুমন ভাই আমাদের প্রিয় একটি মানুস তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়   \n1841                                                                                                                                                                                                                                               আমাকে মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন   \n312                                                                                                                                                                                                                                   তালেবান কে স্যালুট জানাই সারাবিশ্বে আমাকে মত তারাই হকের পথে   \n860                                                                                                                                                                                                                                  ভাইয়ের জন্য সবাই দোয়া করবােন ভাইয়ের জন্য সবাই দোয়া করবেন   \n852                                                                                                                                                                                                                                           থ্যাংক ইউ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য   \n971                                                                                                                                                                                                                                                       থ্যাংক ইউ এতো সুন্দর ধারনা দেওয়ার জন্য   \n1544                                                                                                                                                                                                                                                                    তখনকার মানুস বুঝতে পেরেছে   \n325                                                                                                                                                                                                                                                থ্যাংক ইউ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য   \n849                                                                                                                                                                                                                                                 বিবিসিকে থ্যাংক ইউ জানায় সত্যি কথা বলার জন্য   \n1547                                                                                                                                                                                                                                থ্যাংক ইউ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে   \n844                                                                                                                                                                                                                                          এটা কি একাত্তর টিভির সংবাদ আমাকে তো বিশ্বাস হচ্ছে না   \n842                                                                                                                                                                                                                              সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই থ্যাংক ইউ   \n838                                                                                                                                                                                                                                                 পৃথিবী ছাড়া আর কোথাও কি মানুসের অস্তিত্ব আছে   \n\n      Error        Category  \\\n858       1     Grammatical   \n874       1        Spelling   \n872       1     Grammatical   \n871       1     Grammatical   \n869       1        Spelling   \n309       1        Spelling   \n1542      1        Spelling   \n1841      1     Grammatical   \n312       1     Grammatical   \n860       1     Grammatical   \n852       1  Code Switching   \n971       1  Code Switching   \n1544      1        Spelling   \n325       1  Code Switching   \n849       1  Code Switching   \n1547      1  Code Switching   \n844       1     Grammatical   \n842       1  Code Switching   \n838       1        Spelling   \n\n                                                                                                                                                                                                                                                                                     Correct Form  \\\n858                                                                                                                                                                                                                                                          আমার এজমা আছে আমি কি টিকা দিতে পারবো   \n874                                                                                                                                                                                                                                                 ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে ধরলেন   \n872                                                                                                                                                                                                                                             স্টার জলসা আর জি বাংলা নষ্ট করলো আমার সোনার বাংলা   \n871                                                                                                                                                                                                                                          দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবে   \n869                                                                                                                                                                                                                                                  আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহ নেই   \n309   ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুষের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই   \n1542                                                                                                                                                                                                                 সুমন ভাই আমাদের প্রিয় একটি মানুষ তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়   \n1841                                                                                                                                                                                                                                                আমার মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন   \n312                                                                                                                                                                                                                                    তালেবান কে স্যালুট জানাই সারাবিশ্বে আমার মত তারাই হকের পথে   \n860                                                                                                                                                                                                                                   ভাইয়ের জন্য সবাই দোয়া করবেন ভাইয়ের জন্য সবাই দোয়া করবেন   \n852                                                                                                                                                                                                                                             ধন্যবাদ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য   \n971                                                                                                                                                                                                                                                         ধন্যবাদ এতো সুন্দর ধারনা দেওয়ার জন্য   \n1544                                                                                                                                                                                                                                                                    তখনকার মানুষ বুঝতে পেরেছে   \n325                                                                                                                                                                                                                                                  ধন্যবাদ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য   \n849                                                                                                                                                                                                                                                   বিবিসিকে ধন্যবাদ জানায় সত্যি কথা বলার জন্য   \n1547                                                                                                                                                                                                                                  ধন্যবাদ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে   \n844                                                                                                                                                                                                                                           এটা কি একাত্তর টিভির সংবাদ আমার তো বিশ্বাস হচ্ছে না   \n842                                                                                                                                                                                                                                সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই ধন্যবাদ   \n838                                                                                                                                                                                                                                                 পৃথিবী ছাড়া আর কোথাও কি মানুষের অস্তিত্ব আছে   \n\n      input_token_len  ROUGE-L F1  \\\n858                10         1.0   \n874                12         1.0   \n872                11         1.0   \n871                12         1.0   \n869                12         1.0   \n309                57         1.0   \n1542               16         1.0   \n1841               12         1.0   \n312                11         1.0   \n860                12         1.0   \n852                10         1.0   \n971                 8         1.0   \n1544                6         1.0   \n325                 9         1.0   \n849                10         1.0   \n1547                8         1.0   \n844                11         1.0   \n842                12         1.0   \n838                10         1.0   \n\n                                                                                                                                                                                                                                                                                   Predicted Form  \n858                                                                                                                                                                                                                                                          আমার এজমা আছে আমি কি টিকা দিতে পারবো  \n874                                                                                                                                                                                                                                                 ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে ধরলেন  \n872                                                                                                                                                                                                                                             স্টার জলসা আর জি বাংলা নষ্ট করলো আমার সোনার বাংলা  \n871                                                                                                                                                                                                                                          দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবে  \n869                                                                                                                                                                                                                                                  আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহ নেই  \n309   ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুষের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই  \n1542                                                                                                                                                                                                                 সুমন ভাই আমাদের প্রিয় একটি মানুষ তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়  \n1841                                                                                                                                                                                                                                                আমার মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন  \n312                                                                                                                                                                                                                                    তালেবান কে স্যালুট জানাই সারাবিশ্বে আমার মত তারাই হকের পথে  \n860                                                                                                                                                                                                                                   ভাইয়ের জন্য সবাই দোয়া করবেন ভাইয়ের জন্য সবাই দোয়া করবেন  \n852                                                                                                                                                                                                                                             ধন্যবাদ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য  \n971                                                                                                                                                                                                                                                         ধন্যবাদ এতো সুন্দর ধারনা দেওয়ার জন্য  \n1544                                                                                                                                                                                                                                                                    তখনকার মানুষ বুঝতে পেরেছে  \n325                                                                                                                                                                                                                                                  ধন্যবাদ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য  \n849                                                                                                                                                                                                                                                   বিবিসিকে ধন্যবাদ জানায় সত্যি কথা বলার জন্য  \n1547                                                                                                                                                                                                                                  ধন্যবাদ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে  \n844                                                                                                                                                                                                                                           এটা কি একাত্তর টিভির সংবাদ আমার তো বিশ্বাস হচ্ছে না  \n842                                                                                                                                                                                                                                সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই ধন্যবাদ  \n838                                                                                                                                                                                                                                                 পৃথিবী ছাড়া আর কোথাও কি মানুষের অস্তিত্ব আছে  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video Title</th>\n      <th>Genre</th>\n      <th>Comment</th>\n      <th>Error</th>\n      <th>Category</th>\n      <th>Correct Form</th>\n      <th>input_token_len</th>\n      <th>ROUGE-L F1</th>\n      <th>Predicted Form</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>858</th>\n      <td>শেখ মুজিব হত্যার পর জেনারেল জিয়া যে মন্তব্য করেছিলেন</td>\n      <td>Politics</td>\n      <td>আমাকে এজমা আছে আমি কি টিকা দিতে পারবো</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>আমার এজমা আছে আমি কি টিকা দিতে পারবো</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>আমার এজমা আছে আমি কি টিকা দিতে পারবো</td>\n    </tr>\n    <tr>\n      <th>874</th>\n      <td>বাধ্য হয়েই সুযোগ দেয়া হয়েছে তামিমকে ? | Tamim Iqbal | Riyad | Cricket | Ekattor TV</td>\n      <td>Sports</td>\n      <td>ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে দরলেন</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে ধরলেন</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>ধন্যবাদ ভাই রিসাদ আজিম ভাই সত্য টা তুলে ধরলেন</td>\n    </tr>\n    <tr>\n      <th>872</th>\n      <td>তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla</td>\n      <td>News</td>\n      <td>স্টার জলসা আর জি বাংলা নষ্ট করলো আমাকে সোনার বাংলা</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>স্টার জলসা আর জি বাংলা নষ্ট করলো আমার সোনার বাংলা</td>\n      <td>11</td>\n      <td>1.0</td>\n      <td>স্টার জলসা আর জি বাংলা নষ্ট করলো আমার সোনার বাংলা</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>আলু দিয়ে মুরগির ঝোল বানানোর সেরা পদ্ধতি| New style chicken curry recipe in bengali |Atanur Rannaghar</td>\n      <td>Miscellaneous</td>\n      <td>দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবা</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবে</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>দাদা সরিষা তেল না দিয়ে সয়াবিন তেল ব্যবহার করা যাবে</td>\n    </tr>\n    <tr>\n      <th>869</th>\n      <td>আইপিএলের নিলাম তালিকায় পাঁচ বাংলাদেশি | IPL | Bangladeshi | Khelajog | Ekattor TV</td>\n      <td>Sports</td>\n      <td>আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহো নেই</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহ নেই</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>আর কেউ না খেললেও সাকিব খেলবে তাতে সন্দেহ নেই</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>Turkey earthquake: সিরিয়ার ধ্বংসস্তূপ থেকে সুস্থ নবজাতক উদ্ধার</td>\n      <td>News</td>\n      <td>ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুসের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুষের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই</td>\n      <td>57</td>\n      <td>1.0</td>\n      <td>ইনশাআল্লাহ আমাদের দেশ তুস্কের মতো হবে না ইনশাআল্লাহ লাখো লাখো আলেমের দোয়া এবং চোখের পানি তাহাজ্জুদ নামাজের মতো গুরুত্বপূর্ণ আমল আছে অনেক আল্লাহ ওলার দোয়া আছে আমাদের দেশের মানুষের উপর আমরা আল্লাহ উপর নির্ভর করে থাকি আসুন সবাই মিলে আল্লাহর কাছে নামাজ পড়ে দোয়া করি এবং ভালো হয়ে যাই</td>\n    </tr>\n    <tr>\n      <th>1542</th>\n      <td>বাহুবলী সিনেমার শ্যুটিং হয়েছিলো কেরালার যে জলপ্রপাতে || Bahubali Waterfalls | Athirapilly Waterfalls</td>\n      <td>Entertainment</td>\n      <td>সুমন ভাই আমাদের প্রিয় একটি মানুস তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>সুমন ভাই আমাদের প্রিয় একটি মানুষ তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>সুমন ভাই আমাদের প্রিয় একটি মানুষ তার ব্লগ দেখে আমাদের প্রাণটা জুড়িয়ে যায়</td>\n    </tr>\n    <tr>\n      <th>1841</th>\n      <td>Oporadhi | Ankur Mahamud Feat Arman Alif | Bangla Song 2018 | Official Video</td>\n      <td>Entertainment</td>\n      <td>আমাকে মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>আমার মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>আমার মত কে কে আছেন এখনো সময় পেলে গানটা শুনেন</td>\n    </tr>\n    <tr>\n      <th>312</th>\n      <td>করোনা ভাইরাস ভ্যাকসিন: টিকা নিতে আগ্রহী হলে যেসব বিষয় আপনার জানা থাকা জরুরি</td>\n      <td>News</td>\n      <td>তালেবান কে স্যালুট জানাই সারাবিশ্বে আমাকে মত তারাই হকের পথে</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>তালেবান কে স্যালুট জানাই সারাবিশ্বে আমার মত তারাই হকের পথে</td>\n      <td>11</td>\n      <td>1.0</td>\n      <td>তালেবান কে স্যালুট জানাই সারাবিশ্বে আমার মত তারাই হকের পথে</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla</td>\n      <td>News</td>\n      <td>ভাইয়ের জন্য সবাই দোয়া করবােন ভাইয়ের জন্য সবাই দোয়া করবেন</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>ভাইয়ের জন্য সবাই দোয়া করবেন ভাইয়ের জন্য সবাই দোয়া করবেন</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>ভাইয়ের জন্য সবাই দোয়া করবেন ভাইয়ের জন্য সবাই দোয়া করবেন</td>\n    </tr>\n    <tr>\n      <th>852</th>\n      <td>Khulna Travel Vlog | খুলনা ভ্রমন গল্প</td>\n      <td>Entertainment</td>\n      <td>থ্যাংক ইউ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ধন্যবাদ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>ধন্যবাদ খুলনাকে এতো সুন্দরভাবে উপস্থাপন করার জন্য</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>যশোরে গ্রামবাসীরা তৈরি করছে হাজার ফুট দীর্ঘ ভাসমান সেতু| BBC News Bangla</td>\n      <td>News</td>\n      <td>থ্যাংক ইউ এতো সুন্দর ধারনা দেওয়ার জন্য</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ধন্যবাদ এতো সুন্দর ধারনা দেওয়ার জন্য</td>\n      <td>8</td>\n      <td>1.0</td>\n      <td>ধন্যবাদ এতো সুন্দর ধারনা দেওয়ার জন্য</td>\n    </tr>\n    <tr>\n      <th>1544</th>\n      <td>তুরস্কের চেয়ে ভয়াবহ ভূমিকম্পের ঝুঁকিতে বাংলাদেশ | BBC Bangla</td>\n      <td>News</td>\n      <td>তখনকার মানুস বুঝতে পেরেছে</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>তখনকার মানুষ বুঝতে পেরেছে</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>তখনকার মানুষ বুঝতে পেরেছে</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>Barishal । বরিশাল । Barishal Travel Vlog । History of Barishal Bangladesh । Barishal Vromon Guide</td>\n      <td>Entertainment</td>\n      <td>থ্যাংক ইউ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ধন্যবাদ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>ধন্যবাদ ভাই বরিশালের সৌন্দর্য তুলে ধরার জন্য</td>\n    </tr>\n    <tr>\n      <th>849</th>\n      <td>বিএনপির সঙ্গে ৩২টি দল- সরকারের জন্য কতটা হুমকি? | BBC Bangla</td>\n      <td>Politics</td>\n      <td>বিবিসিকে থ্যাংক ইউ জানায় সত্যি কথা বলার জন্য</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>বিবিসিকে ধন্যবাদ জানায় সত্যি কথা বলার জন্য</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>বিবিসিকে ধন্যবাদ জানায় সত্যি কথা বলার জন্য</td>\n    </tr>\n    <tr>\n      <th>1547</th>\n      <td>ডিবি হারুনের প্রশংসা করলেন হিরো আলম | Hero Alom | DB Police | Channel 24</td>\n      <td>News</td>\n      <td>থ্যাংক ইউ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ধন্যবাদ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে</td>\n      <td>8</td>\n      <td>1.0</td>\n      <td>ধন্যবাদ মাননীয় প্রধানমন্ত্রীকে ধন্যবাদ আইনশৃঙ্খলা বাহিনীকে</td>\n    </tr>\n    <tr>\n      <th>844</th>\n      <td>ডলারকে বাদ দেয়ার জন্য কি কি চেষ্টা করে যাচ্ছে ব্রিকস জোট ? | Dollar | News | Ekattor TV</td>\n      <td>Politics</td>\n      <td>এটা কি একাত্তর টিভির সংবাদ আমাকে তো বিশ্বাস হচ্ছে না</td>\n      <td>1</td>\n      <td>Grammatical</td>\n      <td>এটা কি একাত্তর টিভির সংবাদ আমার তো বিশ্বাস হচ্ছে না</td>\n      <td>11</td>\n      <td>1.0</td>\n      <td>এটা কি একাত্তর টিভির সংবাদ আমার তো বিশ্বাস হচ্ছে না</td>\n    </tr>\n    <tr>\n      <th>842</th>\n      <td>বিএনপির সমাবেশে গণগ্রেফতার নিয়ে যে বার্তা দিল যুক্তরাষ্ট্র | BNP | USA | BD Politics | ATN News</td>\n      <td>Politics</td>\n      <td>সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই থ্যাংক ইউ</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই ধন্যবাদ</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>সাংবাদিকরা এখন নিরপেক্ষভাবে কাজ করছে তাই তাদেরকে জানাই ধন্যবাদ</td>\n    </tr>\n    <tr>\n      <th>838</th>\n      <td>ছায়াপথ বা গ্যালাক্সি | কি কেন কিভাবে | Galaxy | Ki Keno Kivabe</td>\n      <td>Miscellaneous</td>\n      <td>পৃথিবী ছাড়া আর কোথাও কি মানুসের অস্তিত্ব আছে</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>পৃথিবী ছাড়া আর কোথাও কি মানুষের অস্তিত্ব আছে</td>\n      <td>10</td>\n      <td>1.0</td>\n      <td>পৃথিবী ছাড়া আর কোথাও কি মানুষের অস্তিত্ব আছে</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"e_df.tail(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T22:35:28.276955Z","iopub.execute_input":"2023-09-10T22:35:28.277348Z","iopub.status.idle":"2023-09-10T22:35:28.308110Z","shell.execute_reply.started":"2023-09-10T22:35:28.277314Z","shell.execute_reply":"2023-09-10T22:35:28.306963Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"                                                                                               Video Title  \\\n313     বিদ্যুৎ কেন নাই ব্যাখ্যা দিলেন প্রধানমন্ত্রী | Load Shedding Bangladesh | Sheikh Hasina | Somoy TV   \n354      ইতিহাসের সবথেকে ভয়ংকর দুর্গ | যেখানে একবার ঢুকলে কেউ আর ফেরত আসতো না | History of Daulatabad Fort   \n1367                                     আমি বিশ্বের সবচেয়ে উষ্ণ স্থানে গিয়েছিলাম (৭০.৭°সে.) লুত মরুভূমি   \n1493                                                                  Xiaomi 13 Lite - এমন ফোনই আমরা চাই !   \n335                 পশুদের মজার কর্মকান্ড ক্যামেরায় ধরা পড়া | Funny Animals Video 2022 (Part-3) | mayajaal   \n1403              SURONGO | Official Foretaste | Afran Nisho | Tama Mirza | Raihan Rafi | Alpha-i | Chorki   \n1595                                             Nothing Phone 1 First Time Unboxing & Impression 🇧🇩 | ATC   \n394                  ঋতু পরিবর্তন এবং মরুভূমি Season change on earth and Desert explained in Bangla Ep 107   \n1675                             [২য় পর্ব] সেরা কয়েকটি দৃষ্টিভ্রম | Top optical and sound illusion bangla|   \n342                                         ছায়াপথ বা গ্যালাক্সি | কি কেন কিভাবে | Galaxy | Ki Keno Kivabe   \n733                       Notun Bou | নতুন বউ | Natok | Apurba | Sabila Nur | Rubel Hasan | New Natok 2023   \n1280                                                                  Xiaomi 13 Lite - এমন ফোনই আমরা চাই !   \n612   হিরো আলম - শামীম ওসমান ইস্যুতে যে বার্তা দিল যুক্তরাষ্ট্র | Hero Alam | Shamim Osman | USA |ATN News   \n578               SURONGO | Official Foretaste | Afran Nisho | Tama Mirza | Raihan Rafi | Alpha-i | Chorki   \n1696         জিয়াউর রহমান: বিএনপির প্রতিষ্ঠাতা ও সাবেক রাষ্ট্রপতির মৃতদেহের খোঁজ মিলে যেভাবে | BBC Bangla   \n126             মাহমুদুল্লাহর সেরা ১০টি ইনিংস || 10 Greatest Innings of Mahmudullah Riyad || Bissoy Bangla   \n191                                                                                  Pori - Bappa Mazumder   \n7     হৃদয় ছুঁয়ে যাওয়া ৭টি সেরা ইমোশনাল বিজ্ঞাপন | Top 7 Most Emotional Bangladesh Ads Compilation | HD   \n868   মনে পড়ে? ৮০ দশকের বিটিভির সেই সুর। 80's morning show music of Bangladesh Television (BTV)। Life_2711   \n1782                          Oporadhi | Ankur Mahamud Feat Arman Alif | Bangla Song 2018 | Official Video   \n\n              Genre  \\\n313            News   \n354   Miscellaneous   \n1367  Miscellaneous   \n1493  Miscellaneous   \n335   Entertainment   \n1403  Entertainment   \n1595  Miscellaneous   \n394   Miscellaneous   \n1675  Miscellaneous   \n342   Miscellaneous   \n733   Entertainment   \n1280  Miscellaneous   \n612        Politics   \n578   Entertainment   \n1696       Politics   \n126          Sports   \n191   Entertainment   \n7     Entertainment   \n868   Entertainment   \n1782  Entertainment   \n\n                                                             Comment  Error  \\\n313                                  কয়লা আছে ঠিকই মাগার ট্যেকা নাই      1   \n354           এই কেল্লা সত্যিই সিভিল ইঞ্জিনিয়ারিং এর আশ্চর্য ইতিহাস      1   \n1367                                অসাধারণ একটি ডকুমেন্টারি চ্যানেল      1   \n1493  শাওমির ফোন আমারে ফ্রী দিলেও তো আমি ইউজ করবো থার্ড ক্লাস মার্কা      1   \n335                                      ডায়লোগ গুলো বেশি বেশি হইছে      1   \n1403                           এইটা কোন মুভির ট্রেইলার রইল একটা নাটক      1   \n1595                      মিড বাজেটে ওয়াটারপ্রুফ ফোন কোনটা ভালো হবে      1   \n394                                                   প্রথম ভিও আমার      1   \n1675                                         এমন ভিডিও আরো চাই প্লিজ      1   \n342                    ভাইয়া ব্লকহোল নিয়ে একটি ভিডিও বানাবেন প্লিজ      1   \n733            নাটকে বিদেশী লোকেশন থেকে বাংলাদেশের লোকেশন অনেক বেটার      1   \n1280                                  ভাই কত হিট হলে হিট ইসু দরা হয়      1   \n612                          হিরো সব সময় হিরোই থাকি হিরো আলমই বেস্ট      1   \n578                                          ভাইরে ভাই কি এক্সপ্রেসন      1   \n1696                                              বোন তোমাকে স্যালোট      1   \n126                   রিয়েল হিরু সাইলেন্ট কিলার মাহমুদুল্লাহ রিয়াদ      1   \n191                             বাপ্পা মজুমদার জাস্ট মাইন্ড ব্লোয়িং      1   \n7                                   খুব মিস করি সেইসব ছোট বেলার সময়      1   \n868                                        অনেক মিস করি আগের দিনগুলো      1   \n1782                                                       আই মিস ইউ      1   \n\n             Category  \\\n313          Spelling   \n354    Code Switching   \n1367   Code Switching   \n1493   Code Switching   \n335          Spelling   \n1403   Code Switching   \n1595   Code Switching   \n394    Code Switching   \n1675   Code Switching   \n342    Code Switching   \n733    Code Switching   \n1280  Multiple Errors   \n612   Multiple Errors   \n578    Code Switching   \n1696   Code Switching   \n126    Code Switching   \n191    Code Switching   \n7      Code Switching   \n868    Code Switching   \n1782   Code Switching   \n\n                                                                   Correct Form  \\\n313                                              কয়লা আছে ঠিকই কিন্তু টাকা নাই   \n354                         এই কেল্লা সত্যিই নির্মাণ প্রকৌশলী এর আশ্চর্য ইতিহাস   \n1367                                             অসাধারণ একটি তথ্যচিত্র চ্যানেল   \n1493  শাওমির ফোন আমাকে বিনামূল্যে দিলেও তো আমি ব্যবহার করবো তৃতীয় ক্লাস মার্কা   \n335                                                 সংলাপ গুলো বেশি বেশি হয়েছে   \n1403                                    এইটা কোন চলচ্চিত্রের খন্ড রইল একটা নাটক   \n1595                                      মধ্য বাজেটে জলরোধী ফোন কোনটা ভালো হবে   \n394                                                            প্রথম দর্শন আমার   \n1675                                              এমন ভিডিও আরো চাই অনুগ্রহ করে   \n342                     ভাইয়া কৃষ্ণ গহ্বর নিয়ে একটি ভিডিও বানাবেন অনুগ্রহ করে   \n733                     নাটকে বিদেশী অবস্থান থেকে বাংলাদেশের অবস্থান অনেক উত্তম   \n1280                                          ভাই কত তাপ হলে তাপ সমস্যা ধরা হয়   \n612                                  নায়ক সব সময় নায়কই থাকে হিরো আলমই সবসেরা   \n578                                                      ভাইরে ভাই কি অভিবেক্তি   \n1696                                                         বোন তোমাকে অভিবাদন   \n126                                     আসল নায়ক নীরব খুনি মাহমুদুল্লাহ রিয়াদ   \n191                                                  বাপ্পা মজুমদার শুধু চমৎকার   \n7                                                সেসব ছোটবেলার সময় খুব মনে পরে   \n868                                                আগের দিনগুলোর অভাব অনুভব করি   \n1782                                                      আমার তোমাকে মনে পড়ছে   \n\n      input_token_len  ROUGE-L F1  \\\n313                10    0.754098   \n354                 9    0.742857   \n1367                5    0.741935   \n1493               16    0.740741   \n335                 8    0.740741   \n1403                9    0.736842   \n1595               11    0.734177   \n394                 5    0.733333   \n1675                6    0.730769   \n342                10    0.720000   \n733                 9    0.703704   \n1280               11    0.666667   \n612                11    0.666667   \n578                 7    0.666667   \n1696                6    0.666667   \n126                 9    0.635294   \n191                 8    0.580645   \n7                   8    0.580645   \n868                 6    0.452830   \n1782                4    0.266667   \n\n                                                      Predicted Form  \n313                                  কয়লা আছে ঠিকই মাগার ট্যেকা নাই  \n354           এই কেল্লা সত্যিই সিভিল ইঞ্জিনিয়ারিং এর আশ্চর্য ইতিহাস  \n1367                                অসাধারণ একটি ডকুমেন্টারি চ্যানেল  \n1493  শাওমির ফোন আমারে ফ্রী দিলেও তো আমি ইউজ করবো থার্ড ক্লাস মার্কা  \n335                                      ডায়লোগ গুলো বেশি বেশি হইছে  \n1403                           এইটা কোন মুভির ট্রেইলার রইল একটা নাটক  \n1595                      মিড বাজেটে ওয়াটারপ্রুফ ফোন কোনটা ভালো হবে  \n394                                                   প্রথম ভিও আমার  \n1675                                         এমন ভিডিও আরো চাই প্লিজ  \n342                    ভাইয়া ব্লকহোল নিয়ে একটি ভিডিও বানাবেন প্লিজ  \n733            নাটকে বিদেশী লোকেশন থেকে বাংলাদেশের লোকেশন অনেক বেটার  \n1280                                  ভাই কত হিট হলে হিট ইসু দরা হয়  \n612                          হিরো সব সময় হিরোই থাকি হিরো আলমই বেস্ট  \n578                                          ভাইরে ভাই কি এক্সপ্রেসন  \n1696                                              বোন তোমাকে স্যালোট  \n126                   রিয়েল হিরু সাইলেন্ট কিলার মাহমুদুল্লাহ রিয়াদ  \n191                             বাপ্পা মজুমদার জাস্ট মাইন্ড ব্লোয়িং  \n7                                   খুব মিস করি সেইসব ছোট বেলার সময়  \n868                                        অনেক মিস করি আগের দিনগুলো  \n1782                                                       আই মিস ইউ  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video Title</th>\n      <th>Genre</th>\n      <th>Comment</th>\n      <th>Error</th>\n      <th>Category</th>\n      <th>Correct Form</th>\n      <th>input_token_len</th>\n      <th>ROUGE-L F1</th>\n      <th>Predicted Form</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313</th>\n      <td>বিদ্যুৎ কেন নাই ব্যাখ্যা দিলেন প্রধানমন্ত্রী | Load Shedding Bangladesh | Sheikh Hasina | Somoy TV</td>\n      <td>News</td>\n      <td>কয়লা আছে ঠিকই মাগার ট্যেকা নাই</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>কয়লা আছে ঠিকই কিন্তু টাকা নাই</td>\n      <td>10</td>\n      <td>0.754098</td>\n      <td>কয়লা আছে ঠিকই মাগার ট্যেকা নাই</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>ইতিহাসের সবথেকে ভয়ংকর দুর্গ | যেখানে একবার ঢুকলে কেউ আর ফেরত আসতো না | History of Daulatabad Fort</td>\n      <td>Miscellaneous</td>\n      <td>এই কেল্লা সত্যিই সিভিল ইঞ্জিনিয়ারিং এর আশ্চর্য ইতিহাস</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>এই কেল্লা সত্যিই নির্মাণ প্রকৌশলী এর আশ্চর্য ইতিহাস</td>\n      <td>9</td>\n      <td>0.742857</td>\n      <td>এই কেল্লা সত্যিই সিভিল ইঞ্জিনিয়ারিং এর আশ্চর্য ইতিহাস</td>\n    </tr>\n    <tr>\n      <th>1367</th>\n      <td>আমি বিশ্বের সবচেয়ে উষ্ণ স্থানে গিয়েছিলাম (৭০.৭°সে.) লুত মরুভূমি</td>\n      <td>Miscellaneous</td>\n      <td>অসাধারণ একটি ডকুমেন্টারি চ্যানেল</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>অসাধারণ একটি তথ্যচিত্র চ্যানেল</td>\n      <td>5</td>\n      <td>0.741935</td>\n      <td>অসাধারণ একটি ডকুমেন্টারি চ্যানেল</td>\n    </tr>\n    <tr>\n      <th>1493</th>\n      <td>Xiaomi 13 Lite - এমন ফোনই আমরা চাই !</td>\n      <td>Miscellaneous</td>\n      <td>শাওমির ফোন আমারে ফ্রী দিলেও তো আমি ইউজ করবো থার্ড ক্লাস মার্কা</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>শাওমির ফোন আমাকে বিনামূল্যে দিলেও তো আমি ব্যবহার করবো তৃতীয় ক্লাস মার্কা</td>\n      <td>16</td>\n      <td>0.740741</td>\n      <td>শাওমির ফোন আমারে ফ্রী দিলেও তো আমি ইউজ করবো থার্ড ক্লাস মার্কা</td>\n    </tr>\n    <tr>\n      <th>335</th>\n      <td>পশুদের মজার কর্মকান্ড ক্যামেরায় ধরা পড়া | Funny Animals Video 2022 (Part-3) | mayajaal</td>\n      <td>Entertainment</td>\n      <td>ডায়লোগ গুলো বেশি বেশি হইছে</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>সংলাপ গুলো বেশি বেশি হয়েছে</td>\n      <td>8</td>\n      <td>0.740741</td>\n      <td>ডায়লোগ গুলো বেশি বেশি হইছে</td>\n    </tr>\n    <tr>\n      <th>1403</th>\n      <td>SURONGO | Official Foretaste | Afran Nisho | Tama Mirza | Raihan Rafi | Alpha-i | Chorki</td>\n      <td>Entertainment</td>\n      <td>এইটা কোন মুভির ট্রেইলার রইল একটা নাটক</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>এইটা কোন চলচ্চিত্রের খন্ড রইল একটা নাটক</td>\n      <td>9</td>\n      <td>0.736842</td>\n      <td>এইটা কোন মুভির ট্রেইলার রইল একটা নাটক</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>Nothing Phone 1 First Time Unboxing &amp; Impression 🇧🇩 | ATC</td>\n      <td>Miscellaneous</td>\n      <td>মিড বাজেটে ওয়াটারপ্রুফ ফোন কোনটা ভালো হবে</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>মধ্য বাজেটে জলরোধী ফোন কোনটা ভালো হবে</td>\n      <td>11</td>\n      <td>0.734177</td>\n      <td>মিড বাজেটে ওয়াটারপ্রুফ ফোন কোনটা ভালো হবে</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>ঋতু পরিবর্তন এবং মরুভূমি Season change on earth and Desert explained in Bangla Ep 107</td>\n      <td>Miscellaneous</td>\n      <td>প্রথম ভিও আমার</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>প্রথম দর্শন আমার</td>\n      <td>5</td>\n      <td>0.733333</td>\n      <td>প্রথম ভিও আমার</td>\n    </tr>\n    <tr>\n      <th>1675</th>\n      <td>[২য় পর্ব] সেরা কয়েকটি দৃষ্টিভ্রম | Top optical and sound illusion bangla|</td>\n      <td>Miscellaneous</td>\n      <td>এমন ভিডিও আরো চাই প্লিজ</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>এমন ভিডিও আরো চাই অনুগ্রহ করে</td>\n      <td>6</td>\n      <td>0.730769</td>\n      <td>এমন ভিডিও আরো চাই প্লিজ</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>ছায়াপথ বা গ্যালাক্সি | কি কেন কিভাবে | Galaxy | Ki Keno Kivabe</td>\n      <td>Miscellaneous</td>\n      <td>ভাইয়া ব্লকহোল নিয়ে একটি ভিডিও বানাবেন প্লিজ</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ভাইয়া কৃষ্ণ গহ্বর নিয়ে একটি ভিডিও বানাবেন অনুগ্রহ করে</td>\n      <td>10</td>\n      <td>0.720000</td>\n      <td>ভাইয়া ব্লকহোল নিয়ে একটি ভিডিও বানাবেন প্লিজ</td>\n    </tr>\n    <tr>\n      <th>733</th>\n      <td>Notun Bou | নতুন বউ | Natok | Apurba | Sabila Nur | Rubel Hasan | New Natok 2023</td>\n      <td>Entertainment</td>\n      <td>নাটকে বিদেশী লোকেশন থেকে বাংলাদেশের লোকেশন অনেক বেটার</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>নাটকে বিদেশী অবস্থান থেকে বাংলাদেশের অবস্থান অনেক উত্তম</td>\n      <td>9</td>\n      <td>0.703704</td>\n      <td>নাটকে বিদেশী লোকেশন থেকে বাংলাদেশের লোকেশন অনেক বেটার</td>\n    </tr>\n    <tr>\n      <th>1280</th>\n      <td>Xiaomi 13 Lite - এমন ফোনই আমরা চাই !</td>\n      <td>Miscellaneous</td>\n      <td>ভাই কত হিট হলে হিট ইসু দরা হয়</td>\n      <td>1</td>\n      <td>Multiple Errors</td>\n      <td>ভাই কত তাপ হলে তাপ সমস্যা ধরা হয়</td>\n      <td>11</td>\n      <td>0.666667</td>\n      <td>ভাই কত হিট হলে হিট ইসু দরা হয়</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>হিরো আলম - শামীম ওসমান ইস্যুতে যে বার্তা দিল যুক্তরাষ্ট্র | Hero Alam | Shamim Osman | USA |ATN News</td>\n      <td>Politics</td>\n      <td>হিরো সব সময় হিরোই থাকি হিরো আলমই বেস্ট</td>\n      <td>1</td>\n      <td>Multiple Errors</td>\n      <td>নায়ক সব সময় নায়কই থাকে হিরো আলমই সবসেরা</td>\n      <td>11</td>\n      <td>0.666667</td>\n      <td>হিরো সব সময় হিরোই থাকি হিরো আলমই বেস্ট</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>SURONGO | Official Foretaste | Afran Nisho | Tama Mirza | Raihan Rafi | Alpha-i | Chorki</td>\n      <td>Entertainment</td>\n      <td>ভাইরে ভাই কি এক্সপ্রেসন</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>ভাইরে ভাই কি অভিবেক্তি</td>\n      <td>7</td>\n      <td>0.666667</td>\n      <td>ভাইরে ভাই কি এক্সপ্রেসন</td>\n    </tr>\n    <tr>\n      <th>1696</th>\n      <td>জিয়াউর রহমান: বিএনপির প্রতিষ্ঠাতা ও সাবেক রাষ্ট্রপতির মৃতদেহের খোঁজ মিলে যেভাবে | BBC Bangla</td>\n      <td>Politics</td>\n      <td>বোন তোমাকে স্যালোট</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>বোন তোমাকে অভিবাদন</td>\n      <td>6</td>\n      <td>0.666667</td>\n      <td>বোন তোমাকে স্যালোট</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>মাহমুদুল্লাহর সেরা ১০টি ইনিংস || 10 Greatest Innings of Mahmudullah Riyad || Bissoy Bangla</td>\n      <td>Sports</td>\n      <td>রিয়েল হিরু সাইলেন্ট কিলার মাহমুদুল্লাহ রিয়াদ</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>আসল নায়ক নীরব খুনি মাহমুদুল্লাহ রিয়াদ</td>\n      <td>9</td>\n      <td>0.635294</td>\n      <td>রিয়েল হিরু সাইলেন্ট কিলার মাহমুদুল্লাহ রিয়াদ</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>Pori - Bappa Mazumder</td>\n      <td>Entertainment</td>\n      <td>বাপ্পা মজুমদার জাস্ট মাইন্ড ব্লোয়িং</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>বাপ্পা মজুমদার শুধু চমৎকার</td>\n      <td>8</td>\n      <td>0.580645</td>\n      <td>বাপ্পা মজুমদার জাস্ট মাইন্ড ব্লোয়িং</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>হৃদয় ছুঁয়ে যাওয়া ৭টি সেরা ইমোশনাল বিজ্ঞাপন | Top 7 Most Emotional Bangladesh Ads Compilation | HD</td>\n      <td>Entertainment</td>\n      <td>খুব মিস করি সেইসব ছোট বেলার সময়</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>সেসব ছোটবেলার সময় খুব মনে পরে</td>\n      <td>8</td>\n      <td>0.580645</td>\n      <td>খুব মিস করি সেইসব ছোট বেলার সময়</td>\n    </tr>\n    <tr>\n      <th>868</th>\n      <td>মনে পড়ে? ৮০ দশকের বিটিভির সেই সুর। 80's morning show music of Bangladesh Television (BTV)। Life_2711</td>\n      <td>Entertainment</td>\n      <td>অনেক মিস করি আগের দিনগুলো</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>আগের দিনগুলোর অভাব অনুভব করি</td>\n      <td>6</td>\n      <td>0.452830</td>\n      <td>অনেক মিস করি আগের দিনগুলো</td>\n    </tr>\n    <tr>\n      <th>1782</th>\n      <td>Oporadhi | Ankur Mahamud Feat Arman Alif | Bangla Song 2018 | Official Video</td>\n      <td>Entertainment</td>\n      <td>আই মিস ইউ</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>আমার তোমাকে মনে পড়ছে</td>\n      <td>4</td>\n      <td>0.266667</td>\n      <td>আই মিস ইউ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}